# -*- coding: utf-8 -*-
"""Sup3r model software"""
from abc import ABC, abstractmethod
import time
import logging
import pandas as pd
import tensorflow as tf
from tensorflow.keras.metrics import mean_squared_error
from phygnn import CustomNetwork


logger = logging.getLogger(__name__)


class BaseModel(ABC):
    """Abstract base sup3r GAN model."""

    def __init__(self):
        self._gen = None

    @property
    def generator(self):
        """Get the generative model.

        Returns
        -------
        phygnn.base.CustomNetwork
        """
        return self._gen

    @property
    def generator_weights(self):
        """Get a list of layer weights and bias terms for the generator model.

        Returns
        -------
        list
        """
        return self.generator.weights

    def generate(self, low_res, to_numpy=True, training=False):
        """Use the generator model to generate high res data from los res input

        Parameters
        ----------
        low_res : np.ndarray
            Real low-resolution data
        to_numpy : bool
            Flag to convert output from tensor to numpy array
        training : bool
            Flag for predict() used in the training routine. This is used
            to freeze the BatchNormalization and Dropout layers.

        Returns
        -------
        hi_res : np.ndarray
            Synthetically generated high-resolution data
        """
        return self.generator.predict(low_res, to_numpy=to_numpy,
                                      training=training)

    @property
    def history(self):
        """
        Model training history DataFrame (None if not yet trained)

        Returns
        -------
        pandas.DataFrame | None
        """
        return self._history

    @property
    def weights(self):
        """Get a list of all the layer weights and bias terms for the
        generator, spatial discriminator, and temporal discriminator.
        """
        return self.generator_weights

    @abstractmethod
    def run_gradient_descent(self, low_res, hi_res_true):
        """Run gradient descent for one mini-batch of (low_res, hi_res_true)
        and adjust NN weights

        Parameters
        ----------
        low_res : np.ndarray
            Real low-resolution data
        hi_res_true : np.ndarray
            Real high-resolution data

        Returns
        -------
        loss : tf.Tensor
            0D tensor representing the full GAN loss term.
        loss_diagnostics : dict
            Namespace of the breakdown of loss components
        """

    @abstractmethod
    def calc_loss(self, hi_res_true, hi_res_gen):
        """Calculate the GAN loss function using generated and true high
        resolution data.

        Parameters
        ----------
        hi_res_true : tf.Tensor | np.ndarray
            Ground truth high resolution spatiotemporal data.
        hi_res_gen : tf.Tensor
            Superresolved high resolution spatiotemporal data generated by the
            generative model.

        Returns
        -------
        loss : tf.Tensor
            0D tensor representing the full GAN loss term. This can be a
            weighted summation of up to four individual loss terms from the
            generative / discriminative models and their respective spatial /
            temporal components or models.
        loss_diagnostics : dict
            Namespace of the breakdown of loss components
        """

    def train(self, low_res, hi_res, n_batch=None, batch_size=128, n_epoch=100,
              shuffle=True, validation_split=0.1, return_diagnostics=False):
        """Train the GAN model on real low res data and real high res data

        Parameters
        ----------
        low_res : np.ndarray
            Real low-resolution data
        hi_res : np.ndarray
            Real high-resolution data
        n_batch : int
            Number of times to update the weights per epoch (number of
            mini-batches). The training data will be split into this many
            mini-batches and the NN will train on each mini-batch, update
            weights, then move onto the next mini-batch.
        n_epoch : int
            Number of times to iterate on the training data.
        shuffle : bool
            Flag to randomly subset the validation data and batch selection
            from low_res, hi_res
        validation_split : float
            Fraction of low_res and hi_res to use for validation.
        return_diagnostics : bool
            Flag to return training diagnostics dictionary.

        Returns
        -------
        diagnostics : dict
            Namespace of training parameters that can be used for diagnostics.
        """
        epochs = list(range(n_epoch))

        if self._history is None:
            self._history = pd.DataFrame(
                columns=['elapsed_time',
                         'training_loss',
                         'validation_loss',
                         ])
            self._history.index.name = 'epoch'
        else:
            epochs += self._history.index.values[-1] + 1

        val_splits = self.get_val_split(low_res, hi_res, shuffle=shuffle,
                                        validation_split=validation_split)
        low_res, low_res_val = val_splits[0]
        hi_res, hi_res_val = val_splits[1]

        tr_loss = None
        t0 = time.time()
        for epoch in epochs:
            batch_iter = self.make_batches(low_res, hi_res, n_batch=n_batch,
                                           batch_size=batch_size,
                                           shuffle=shuffle)

            for low_res_batch, hi_res_batch in batch_iter:
                tr_loss = self.run_gradient_descent(low_res_batch,
                                                    hi_res_batch)

            hi_res_val_gen = self.generate(low_res_val, to_numpy=False)
            val_loss, _ = self.calc_loss(hi_res_val, hi_res_val_gen)
            logger.info('Epoch {} train loss: {:.2e} '
                        'val loss: {:.2e} for "{}"'
                        .format(epoch, tr_loss, val_loss, self.name))

            self._history.at[epoch, 'elapsed_time'] = time.time() - t0
            self._history.at[epoch, 'training_loss'] = tr_loss.numpy()
            self._history.at[epoch, 'validation_loss'] = val_loss.numpy()

        diagnostics = {'low_res': low_res,
                       'hi_res': hi_res,
                       'low_res_val': low_res_val,
                       'hi_res_val': hi_res_val,
                       'history': self.history,
                       }

        if return_diagnostics:
            return diagnostics


class SpatioTemporalGan:
    """Spatio Temporal Super Resolution GAN model."""

    def __init__(self, gen_layers, disc_t_layers, disc_s_layers):
        """
        Parameters
        ----------
        gen_layers : list | str
            Hidden layers input argument to phygnn.base.CustomNetwork for the
            generative super resolving model. Can also be a str filepath to a
            json config file containing the input layers argument.
        disc_t_layers : list | str
            Hidden layers input argument to phygnn.base.CustomNetwork for the
            discriminative temporal model. Can also be a str filepath to a json
            config file containing the input layers argument.
        disc_s_layers : list | str
            Hidden layers input argument to phygnn.base.CustomNetwork for the
            discriminative spatial model. Can also be a str filepath to a json
            config file containing the input layers argument.
        """

        self._history = None
        self._gen = CustomNetwork(hidden_layers=gen_layers, name='Generator')
        self._disc_t = CustomNetwork(hidden_layers=disc_t_layers,
                                     name='TemporalDiscriminator')
        self._disc_s = CustomNetwork(hidden_layers=disc_s_layers,
                                     name='SpatialDiscriminator')

    @property
    def disc_spatial(self):
        """Get the spatial discriminator model.

        Returns
        -------
        phygnn.base.CustomNetwork
        """
        return self._disc_s

    @property
    def disc_spatial_weights(self):
        """Get a list of layer weights and bias terms for the spatial
        discriminator model.

        Returns
        -------
        list
        """
        return self.disc_spatial.weights

    @property
    def disc_temporal(self):
        """Get the temporal discriminator model.

        Returns
        -------
        phygnn.base.CustomNetwork
        """
        return self._disc_t

    @property
    def disc_temporal_weights(self):
        """Get a list of layer weights and bias terms for the temporal
        discriminator model.

        Returns
        -------
        list
        """
        return self.disc_temporal.weights

    @property
    def weights(self):
        """Get a list of all the layer weights and bias terms for the
        generator, spatial discriminator, and temporal discriminator.
        """
        return (self.generator_weights + self.disc_spatial_weights
                + self.disc_temporal_weights)

    def run_gradient_descent(self, low_res, hi_res_true, weight_gen=1.0,
                             weight_disc_s=1.0, weight_disc_t=1.0):
        """Run gradient descent for one mini-batch of (low_res, hi_res_true)
        and adjust NN weights

        Parameters
        ----------
        low_res : np.ndarray
            Real low-resolution data in a 5D array:
            (n_observations, spatial_1, spatial_2, temporal, features)
        hi_res_true : np.ndarray
            Real high-resolution data in a 5D array:
            (n_observations, spatial_1, spatial_2, temporal, features)
        weight_gen : bool
            Weight factor for the generative loss term in the loss function.
            This includes the generative content loss and the generative
            portion of the discriminator losses (both space and time).
        weight_disc_s : bool
            Weight factor for the spatial discriminator loss term in the loss
            function.
        weight_disc_t : bool
            Weight factor for the temporal discriminator loss term in the loss
            function.

        Returns
        -------
        loss : tf.Tensor
            0D tensor representing the full GAN loss term. This can be a
            weighted summation of up to four individual loss terms from the
            generative / discriminative models and their respective spatial /
            temporal components or models.
        loss_diagnostics : dict
            Namespace of the breakdown of loss components
        """

        with tf.GradientTape() as tape:
            for layer in self._layers:
                tape.watch(layer.variables)

            hi_res_gen = self.generate(low_res, to_numpy=False, training=True)
            loss_out = self.calc_loss(hi_res_true, hi_res_gen,
                                      weight_gen=weight_gen,
                                      weight_disc_s=weight_disc_s,
                                      weight_disc_t=weight_disc_t)
            loss, loss_diagnostics = loss_out

            # do you even lift bro?
            training_weights = []
            if weight_gen > 0:
                training_weights += self.generator_weights
            if weight_disc_s > 0:
                training_weights += self.disc_spatial_weights
            if weight_disc_t > 0:
                training_weights += self.disc_temporal_weights

            grad = tape.gradient(loss, training_weights)

        self._optimizer.apply_gradients(zip(grad, self.weights))

        return loss, loss_diagnostics

    def calc_loss_gen(self, hi_res_true, hi_res_gen, disc_out_spatial,
                      disc_out_temporal):
        """Calculate the loss term for the generator model.

        Parameters
        ----------
        hi_res_true : tf.Tensor | np.ndarray
            Ground truth high resolution spatiotemporal data.
        hi_res_gen : tf.Tensor
            Superresolved high resolution spatiotemporal data generated by the
            generative model.
        disc_out_spatial : tf.Tensor
            Raw discriminator outputs from the spatial discriminator model
            predicting only on hi_res_gen (not on hi_res_true).
        disc_out_temporal : tf.Tensor
            Raw discriminator outputs from the temporal discriminator model
            predicting only on hi_res_gen (not on hi_res_true).

        Returns
        -------
        loss_gen_s : tf.Tensor
            0D tensor generator model loss for the spatial component of the
            super resolution generated output. If self.alpha_advers==0, this is
            just the generator content loss.
        loss_gen_t : tf.Tensor
            0D tensor generator model loss for the temporal component of the
            super resolution generated output. If self.alpha_advers==0, this is
            just the generator content loss.
        """

        loss_gen_content = mean_squared_error(hi_res_true, hi_res_gen)
        loss_gen_content = tf.reduce_mean(loss_gen_content)
        loss_gen_s = loss_gen_content
        loss_gen_t = loss_gen_content

        if self.alpha_advers > 0:
            # note that these have flipped labels from the discriminator
            # loss because of the opposite optimization goal
            loss_disc_s_gen = tf.nn.sigmoid_cross_entropy_with_logits(
                logits=disc_out_spatial,
                labels=tf.ones_like(disc_out_spatial))

            loss_disc_t_gen = tf.nn.sigmoid_cross_entropy_with_logits(
                logits=disc_out_temporal,
                labels=tf.ones_like(disc_out_temporal))

            loss_gen_s += self.alpha_advers * tf.reduce_mean(loss_disc_s_gen)
            loss_gen_t += self.alpha_advers * tf.reduce_mean(loss_disc_t_gen)

        return loss_gen_s, loss_gen_t

    @staticmethod
    def calc_loss_disc(disc_out_true, disc_out_gen):
        """Calculate the loss term for the discriminator model (either the
        spatial or temporal discriminator).

        Parameters
        ----------
        disc_out_true : tf.Tensor
            Raw discriminator outputs from the discriminator model predicting
            only on hi_res_true (not on hi_res_gen).
        disc_out_gen : tf.Tensor
            Raw discriminator outputs from the discriminator model predicting
            only on hi_res_gen (not on hi_res_true).

        Returns
        -------
        loss_disc : tf.Tensor
            0D tensor discriminator model loss for either the spatial or
            temporal component of the super resolution generated output.
        """

        # note that these have flipped labels from the generator
        # loss because of the opposite optimization goal
        logits = tf.concat([disc_out_true, disc_out_gen], axis=0)
        labels = tf.concat([tf.ones_like(disc_out_true),
                            tf.zeros_like(disc_out_gen)], axis=0)

        loss_disc = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,
                                                            labels=labels)
        loss_disc = tf.reduce_mean(loss_disc)

        return loss_disc

    def calc_loss(self, hi_res_true, hi_res_gen, weight_gen=1.0,
                  weight_disc_s=1.0, weight_disc_t=1.0):
        """Calculate the GAN loss function using generated and true high
        resolution data.

        Parameters
        ----------
        hi_res_true : tf.Tensor | np.ndarray
            Ground truth high resolution spatiotemporal data.
        hi_res_gen : tf.Tensor
            Superresolved high resolution spatiotemporal data generated by the
            generative model.
        weight_gen : bool
            Weight factor for the generative loss term in the loss function.
            This includes the generative content loss and the generative
            portion of the discriminator losses (both space and time).
        weight_disc_s : bool
            Weight factor for the spatial discriminator loss term in the loss
            function.
        weight_disc_t : bool
            Weight factor for the temporal discriminator loss term in the loss
            function.

        Returns
        -------
        loss : tf.Tensor
            0D tensor representing the full GAN loss term. This can be a
            weighted summation of up to four individual loss terms from the
            generative / discriminative models and their respective spatial /
            temporal components or models.
        loss_diagnostics : dict
            Namespace of the breakdown of loss components
        """

        loss_gen_s = tf.constant(0.0, dtype=tf.float32)
        loss_gen_t = tf.constant(0.0, dtype=tf.float32)
        loss_disc_s = tf.constant(0.0, dtype=tf.float32)
        loss_disc_t = tf.constant(0.0, dtype=tf.float32)

        disc_out_spatial_true = self.disc_spatial.predict(hi_res_true)
        disc_out_spatial_gen = self.disc_spatial.predict(hi_res_gen)

        disc_out_temporal_true = self.disc_temporal.predict(hi_res_true)
        disc_out_temporal_gen = self.disc_temporal.predict(hi_res_gen)

        if weight_gen > 0:
            loss_gen_s, loss_gen_t = self.calc_loss_gen(hi_res_true,
                                                        hi_res_gen,
                                                        disc_out_spatial_gen,
                                                        disc_out_temporal_gen)

        if weight_disc_s > 0:
            loss_disc_s = self.calc_loss_disc(disc_out_spatial_true,
                                              disc_out_spatial_gen)

        if weight_disc_t > 0:
            loss_disc_t = self.calc_loss_disc(disc_out_temporal_true,
                                              disc_out_temporal_gen)

        loss = (weight_gen * (loss_gen_s + loss_gen_t)
                + weight_disc_s * loss_disc_s
                + weight_disc_t * loss_disc_t)

        loss_diagnostics = {'loss_gen_s': loss_gen_s,
                            'loss_gen_t': loss_gen_t,
                            'loss_disc_s': loss_disc_s,
                            'loss_disc_t': loss_disc_t,
                            }

        return loss, loss_diagnostics
